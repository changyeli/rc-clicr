# -*- coding: utf-8 -*-
"""QANet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L_7KhV2Eki1I4fnotmr--5KBkqZ9O8kW
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x

import tensorflow as tf
print(tf.__version__)
import numpy as np
import scipy
from scipy import spatial
import sklearn
from sklearn.manifold import TSNE
#import tensorflow.compat.v1 as tf
#tf.disable_v2_behavior()

from util import get_record_parser, convert_tokens, evaluate, get_batch_dataset, get_dataset
from layers import initializer, regularizer, residual_block, highway, conv, mask_logits, trilinear, total_params, optimized_trilinear_for_attention

#word_mat2 = np.loadtxt("word2vec_glove.txt",usecols=range(1, 100+1), comments=None)

N = 32 #num_batch
PL = 400 #passage_length
QL = 30 #question_length
nh = 1 #number of heads in multi-head self attention. We use 1 for simplicity.
char_lim = 16 #character_limit
dim = 128 #dimension
dim_char = 32 #character_dimension
test_para_limit = 1000
test_ques_limit = 100
word_mat = np.loadtxt("glove.840B.300d.txt",dtype ='string',delimiter = ' ') 
 
context = tf.placeholder(tf.int32, [None, PL],"context")
quesion = tf.placeholder(tf.int32, [None, QL],"question")
context_char = tf.placeholder(tf.int32, [None, PL, char_lim],"context_char")
question_char = tf.placeholder(tf.int32, [None, QL, char_lim],"question_char")
y1 = tf.placeholder(tf.int32, [None, PL],"answer_index1")
y2 = tf.placeholder(tf.int32, [None, PL],"answer_index2")

word_mat = tf.get_variable("word_mat", initializer=tf.constant(word_mat, dtype=tf.float32), trainable=False)
char_mat = tf.get_variable("char_mat", initializer=tf.constant(char_mat, dtype=tf.float32))

context_char_emb = tf.reshape(tf.nn.embedding_lookup(word_mat, context_char), [N * PL, char_lim, dim_char])
quesion_char_emb = tf.reshape(tf.nn.embedding_lookup(word_mat, question_char), [N * QL, char_lim, dim_char])

context_char_emb = conv(context_char_emb, dim, bias = True, activation = tf.nn.relu, kernel_size = 5, name = "char_conv", reuse = None)
quesion_char_emb = conv(quesion_char_emb, dim, bias = True, activation = tf.nn.relu, kernel_size = 5, name = "char_conv", reuse = True)

context_char_emb = tf.reduce_max(context_char_emb, axis = 1)
quesion_char_emb = tf.reduce_max(quesion_char_emb, axis = 1)

context_char_emb = tf.reshape(context_char_emb, [N, PL, dim])
quesion_char_emb = tf.reshape(quesion_char_emb, [N, QL, dim])

context_emb = tf.nn.embedding_lookup(word_mat, context)
question_emb = tf.nn.embedding_lookup(word_mat, quesion)

context_emb = tf.concat([context_emb, context_char_emb], axis=2)
question_emb = tf.concat([question_emb, quesion_char_emb], axis=2)

context_emb = highway(context_emb, size = dim, scope = "highway", reuse = None)#layers = 2,
question_emb = highway(question_emb, size = dim,  scope = "highway", reuse = True)#layers = 2,

context = residual_block(context_emb, num_blocks = 1, num_conv_layers = 4, kernel_size = 7,
    num_filters = dim, num_heads = nh, scope = "Encoder_Residual_Block", bias = False)
question = residual_block(question_emb, num_blocks = 1, num_conv_layers = 4, kernel_size = 7,
    num_filters = dim, num_heads = nh, scope = "Encoder_Residual_Block",
    reuse = True, # Share the weights between passage and question
    bias = False)

tiled_context = tf.tile(tf.expand_dims(context,2),[1,1,QL,1])
tiled_question = tf.tile(tf.expand_dims(question,1),[1,PL,1,1])
S = trilinear([tiled_context, tiled_question, tiled_context*tiled_question])
# S.shape = [Batch_size, context_length, question_length, dim]
S_ = tf.nn.softmax(S)
S_T = tf.transpose(tf.nn.softmax(S, dim = 1),(0,2,1))
context2question = tf.matmul(S_, question)
question2context = tf.matmul(tf.matmul(S_, S_T), context)
attention_outputs = [context, context2question, context * context2question, context * question2context]

inputs = tf.concat(attention_outputs, axis = -1)
encoder_inputs = [conv(inputs, dim, name = "input_projection")]
for i in range(3):
    encoder_inputs.append(
        residual_block(encoder_inputs[i],
            num_blocks = 7,
            num_conv_layers = 2,
            kernel_size = 5,
            num_filters = dim,
            num_heads = nh,
            scope = "Model_Encoder",
            bias = False,
            reuse = True if i > 0 else None)
        )

# only use the first and second output of stacked encoder for the first answer probability calculation
start_prob = tf.squeeze(conv(tf.concat([encoder_inputs[1], encoder_inputs[2]],axis = -1),1, bias = False, name = "start_pointer"),-1)
# use the first and THIRD output of stacked encoder for the last answer probability calculation
end_prob = tf.squeeze(conv(tf.concat([encoder_inputs[1], encoder_inputs[3]],axis = -1),1, bias = False, name = "end_pointer"), -1)

# We calculate the loss
losses = tf.nn.softmax_cross_entropy_with_logits(
    logits=start_prob, labels=y1)
losses2 = tf.nn.softmax_cross_entropy_with_logits(
    logits=end_prob, labels=y2)
loss = tf.reduce_mean(losses + losses2)

print(end_prob)